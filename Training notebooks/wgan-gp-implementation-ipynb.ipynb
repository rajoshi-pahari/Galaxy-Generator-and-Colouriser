{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8476674,"sourceType":"datasetVersion","datasetId":5055294}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Generate Galaxies using Random Numbers (WGAN-GP)\n    \n---\n    ","metadata":{}},{"cell_type":"markdown","source":"## Practical Part: WGAN-GP\n\n### Dataset\n* We are using [5000 images of Galaxies captured via telescopes](https://www.kaggle.com/datasets/spartificial/low-and-high-resolution-galaxy-images-cgan-data).\n* We are going ONLY going to use the Groud Truth Files for this project.\n\n### Table of Contents of Implementing WGAN-GP\n1. <a href='#1'>Importing dependencies</a>\n2. <a href='#2'>Setting Memory Growth for each GPUs</a>\n3. <a href=\"#3\">Project Setup</a>\n4. <a href='#4'>Dataset Preparation</a>\n5. <a href='#5'>Building Generator and Critic</a>\n6. <a href='#6'>Learning Rate Setup</a>\n7. <a href='#7'>Building Checkpoints</a>\n8. <a href='#8'>Function to Generate Images and Save it after each epoch</a>\n9. <a href='#9'>Build the Training Step for Critic and Generator</a>\n10. <a href='#10'>Training of WGAN-GP</a>\n11. <a href='#11'>Compare Generated Data with Real Data</a>","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Dependencies<section id=\"1\">","metadata":{}},{"cell_type":"code","source":"# Import necessary modules and functions for file handling, timing, and mathematical operations.\nimport os\nimport time\nimport math\n\n# Import NumPy for numerical operations and handling arrays.\nimport numpy as np\n\n# Import matplotlib for plotting graphs and visualizations.\nimport matplotlib.pyplot as plt\n\n# Import clear_output from IPython.display to clear the output in Jupyter notebooks.\nfrom IPython.display import clear_output\n\n# Import TensorFlow and its components for creating and training neural networks.\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Activation, Reshape, LayerNormalization, BatchNormalization\nfrom tensorflow.keras.layers import Input, Dropout, Dense, LeakyReLU, Flatten\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import RandomNormal\n\n# Define a constant for optimizing TensorFlow data loading performance.\nAUTOTUNE = tf.data.experimental.AUTOTUNE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Setting Memory Growth for each GPUs<section id=\"2\">","metadata":{}},{"cell_type":"code","source":"# Step 1: List all physical GPU devices available on the system.\n# This function checks the computer for any GPUs that TensorFlow can use and returns a list of those devices.\ndevices = tf.config.experimental.list_physical_devices(\"GPU\")\n\n# Step 2: Iterate over each GPU device found.\n# The 'devices' list contains all GPUs detected by TensorFlow. We loop through this list to configure each GPU.\nfor device in devices:\n    # Step 3: Set memory growth for each GPU.\n    # Memory growth configuration allows TensorFlow to gradually increase the amount of GPU memory it uses, rather than allocating all available memory at once.\n    # This is useful to avoid running out of memory if other applications are also using the GPU or if TensorFlow only needs a small amount of memory to start.\n    tf.config.experimental.set_memory_growth(device=device, enable=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Project Setup<section id=\"3\">","metadata":{}},{"cell_type":"code","source":"# Define the name of the model. In this case, 'DCGAN' stands for Deep Convolutional Generative Adversarial Network.\nMODEL_NAME = 'DCGAN'\n\n# Define the directory where image data is stored. 'dataset/images/' is the path to the folder containing image files.\nDATA_BASE_DIR = 'dataset/images/'\n\n# Define the output path where results and generated outputs will be saved. It combines 'outputs' directory with the model name.\nOUTPUT_PATH = os.path.join('outputs', MODEL_NAME)\n\n# Define the directory for storing training logs. This path is used by TensorFlow to save logs during training for later analysis.\nTRAIN_LOGDIR = os.path.join(\"logs\", \"tensorflow\", MODEL_NAME, 'train_data')\n\n# Check if the OUTPUT_PATH directory does not exist. If it does not exist, create the directory.\nif not os.path.exists(OUTPUT_PATH):\n    os.makedirs(OUTPUT_PATH)\n\n# Define the target size to which all images will be resized. Here, images will be scaled to 64x64 pixels.\nTARGET_IMG_SIZE = 128 \n\n# Define the batch size, which is the number of images processed together in one iteration of training.\nBATCH_SIZE = 64\n\n# Define the dimension of the noise vector. This vector is input to the generator in the GAN, typically of fixed size.\nNOISE_DIM = 100\n\n# Define a hyperparameter lambda (λ) used for gradient penalty in the training of GANs. It helps stabilize training.\nLAMBDA = 10 \n\n# Define the number of epochs for training. One epoch is one complete pass through the training dataset.\nEPOCHS = 1000\n\n# Define the starting epoch. Training will start from epoch 1.\nCURRENT_EPOCH = 1 \n\n# Define how frequently (in terms of epochs) to save model checkpoints. Here, a checkpoint will be saved every 15 epochs.\nSAVE_EVERY_N_EPOCH = 15 \n\n# Define the number of times to train the critic (discriminator) before training the generator. This helps balance the GAN training.\nN_CRITIC = 3 \n\n# Define the initial learning rate for training. The learning rate determines how much to adjust the weights during training.\nLR = 1e-4\n\n# Define the minimum learning rate. This ensures the learning rate doesn’t go below this value, which can be useful for fine-tuning.\nMIN_LR = 0.000001 \n\n# Define the decay factor for the learning rate. This factor controls how the learning rate decreases over time.\nDECAY_FACTOR = 1.00004 \n\n# Define the number of images to display after the training is completed\nNUM_IMAGES = 36\n\n# Create a file writer object for TensorFlow’s summary logs. This object writes logs to the directory defined in TRAIN_LOGDIR.\nfile_writer = tf.summary.create_file_writer(TRAIN_LOGDIR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Dataset Preparation<section id='4'>","metadata":{}},{"cell_type":"code","source":"# Modify the directory to the path of your dataset\n# Create a TensorFlow dataset object that contains file paths to images. This dataset includes all files with a .jpeg extension in the specified directory.\nlist_ds = tf.data.Dataset.list_files('/kaggle/input/Final Ground Truth/*.jpeg')\n\n# Loop through the dataset and take the first 5 file paths.\n# For each file path in these 5 examples, convert the TensorFlow tensor to a NumPy array and print it. \n# This shows the actual file paths of the images.\nfor f in list_ds.take(5):\n    print(f.numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize(image):\n    '''\n    Normalizes the image pixel values to the range [-1, 1].\n\n    Pixel values in images typically range from 0 to 255. This function rescales \n    them to the range [-1, 1] to make them more suitable for neural network training.\n    \n    Args:\n        image (tf.Tensor): The input image tensor with pixel values in the range [0, 255].\n    \n    Returns:\n        tf.Tensor: The normalized image tensor with pixel values in the range [-1, 1].\n    '''\n    # Convert the image tensor to a float32 type to perform mathematical operations.\n    image = tf.cast(image, tf.float32)\n    \n    # Normalize the image pixel values.\n    # Subtract 127.5 to center the values around 0.\n    # Divide by 127.5 to scale the values to the range [-1, 1].\n    image = (image - 127.5) / 127.5\n    \n    return image\n\ndef resize_and_center_crop(image, target_size):\n    '''\n    Resizes an image to fit within the target size while preserving the aspect ratio,\n    then crops the center of the image to the exact target size.\n\n    Args:\n        image (tf.Tensor): The input image tensor.\n        target_size (tuple): The target size (height, width) for resizing and cropping.\n\n    Returns:\n        tf.Tensor: The resized and center-cropped image tensor.\n    '''\n    target_height, target_width = target_size\n    \n    # Get the current dimensions of the image.\n    img_height, img_width = tf.shape(image)[0], tf.shape(image)[1]\n\n    # Calculate the aspect ratio of the original image and the target size.\n    img_aspect = img_width / img_height\n    target_aspect = target_width / target_height\n\n    if img_aspect > target_aspect:\n        # If the image is wider than the target aspect ratio, resize based on width.\n        new_width = target_width\n        new_height = int(target_width / img_aspect)\n    else:\n        # If the image is taller or the same aspect ratio, resize based on height.\n        new_width = int(target_height * img_aspect)\n        new_height = target_height\n\n    # Resize the image to the new dimensions.\n    image_resized = tf.image.resize(image, [new_height, new_width], method='bicubic', antialias=True)\n    \n    # Calculate the coordinates for center cropping.\n    start_y = (new_height - target_height) // 2\n    start_x = (new_width - target_width) // 2\n\n    # Crop the center of the resized image to the target dimensions.\n    image_cropped = tf.image.crop_to_bounding_box(image_resized, start_y, start_x, target_height, target_width)\n\n    return image_cropped\n\ndef preprocess_image(file_path):\n    '''\n    Preprocesses an image from a file path by reading, decoding, resizing with aspect ratio preservation,\n    center cropping, and normalizing it.\n\n    Args:\n        file_path (str): The file path to the image.\n\n    Returns:\n        tf.Tensor: The preprocessed image tensor ready for input into a neural network.\n    '''\n    # Read the image file from the given file path.\n    image = tf.io.read_file(file_path)\n    \n    # Decode the JPEG image file to a tensor.\n    image = tf.image.decode_jpeg(image, channels=1)\n    \n    # Resize and crop the image while preserving the aspect ratio.\n    #image = resize_and_center_crop(image, (TARGET_IMG_SIZE, TARGET_IMG_SIZE))\n    \n    # Normalize the image to the range [-1, 1].\n    image = normalize(image)\n    \n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the preprocessing function to each image in the dataset\n# Shuffle the dataset with a buffer size of 1000 to ensure randomness\n# Cache the dataset in memory to improve performance\n# Batch the data with the specified batch size for training\n# Prefetch batches to improve performance by preparing data in the background while the model is training\ntrain_data = list_ds.map(preprocess_image).shuffle(1000).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a single batch of images from the train_data dataset.\nsample_img = next(iter(train_data))\n\n# Plot the first image from the batch with a title 'Sample'.\nplt.title('Sample')\n\n# Display the image using matplotlib. \n# The image needs to be converted back to the [0, 1] range for visualization.\n# This involves reversing the normalization done earlier, which was in the range [-1, 1].\n# Multiply by 0.5 and add 0.5 to rescale from [-1, 1] to [0, 1].\n# Use np.clip to ensure that pixel values stay within [0, 1] range, which prevents displaying errors.\nplt.imshow(sample_img[0] * 0.5 + 0.5, cmap='gray')\n\n# Show the plot.\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Building Generator and Critic<section id='5'>","metadata":{}},{"cell_type":"code","source":"def CGAN_generator(input_z_shape=NOISE_DIM):\n    \"\"\"\n    Creates a generator model for a Conditional Generative Adversarial Network (CGAN) using a DCGAN-like architecture.\n    \n    The generator takes a noise vector (usually a random vector) as input and generates synthetic images. \n    The architecture is designed to progressively upsample the noise vector to produce an image with the desired dimensions.\n    \n    Args:\n        input_z_shape (int): The shape of the input noise vector. Default is NOISE_DIM, which represents the dimensionality of the noise vector.\n    \n    Returns:\n        tensorflow.keras.Model: A Keras Model object that represents the generator network.\n    \n    The generator architecture consists of:\n    1. A Dense layer that transforms the noise vector into a high-dimensional tensor.\n    2. A Reshape layer that reshapes this tensor into a 4x4 spatial resolution with 512 channels.\n    3. A series of Conv2DTranspose layers that progressively upsample the image. Each layer increases the spatial resolution (height and width) and reduces the number of channels.\n    4. BatchNormalization layers are applied after each Conv2DTranspose layer to normalize the activations, which helps stabilize and speed up training.\n    5. LeakyReLU activation functions are used to introduce non-linearity and avoid dead neurons.\n    6. The final Conv2DTranspose layer produces an image with 3 channels (RGB) using the 'tanh' activation function to scale pixel values to the range [-1, 1].\n    \"\"\"\n    # Define the input layer with the shape of the noise vector. \n    # This is the input to the generator network.\n    input_z_layer = Input((input_z_shape,))\n    \n    # First, fully connect the noise vector to a high-dimensional tensor.\n    # This tensor will be reshaped into a 4x4 image with 512 channels.\n    z = Dense(4*4*512, use_bias=False)(input_z_layer)\n    z = Reshape((4, 4, 512))(z)\n    \n    # Apply a series of Conv2DTranspose layers to upsample the image.\n    # Each Conv2DTranspose layer increases the spatial dimensions of the image.\n\n    # First transposed convolution layer\n    x = Conv2DTranspose(512, (4, 4), strides=(1, 1), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(z)\n    x = BatchNormalization()(x)  # Normalize the output of the convolution to help stabilize training\n    x = LeakyReLU()(x)  # Apply Leaky ReLU activation function for non-linearity\n    \n    # Second transposed convolution layer\n    x = Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    \n    # Third transposed convolution layer\n    x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    \n    # Fourth transposed convolution layer\n    x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    \n    # Fifth transposed convolution layer\n    x = Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    \n    # Final transposed convolution layer to generate the output image\n    # The output image has 3 channels (RGB) and uses 'tanh' activation to ensure pixel values are between -1 and 1\n    output = Conv2DTranspose(1, (4, 4), strides=(2, 2), padding='same', use_bias=False, activation=\"tanh\",\n                             kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(x)\n    \n    # Create a model with the input and output layers defined above\n    model = Model(inputs=input_z_layer, outputs=output)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CGAN_discriminator(input_x_shape=(TARGET_IMG_SIZE, TARGET_IMG_SIZE, 1)):\n    \"\"\"\n    Creates a discriminator model for a Conditional Generative Adversarial Network (CGAN) using a DCGAN-like architecture.\n    \n    The discriminator takes an image as input and determines whether it is a real image from the dataset or a synthetic image generated by the generator. \n    The architecture is designed to progressively downsample the image to produce a single score indicating its authenticity.\n\n    Args:\n        input_x_shape (tuple): The shape of the input image. Default is (TARGET_IMG_SIZE, TARGET_IMG_SIZE, 3), where \n                               TARGET_IMG_SIZE is the height and width of the image, and 3 represents the RGB color channels.\n    \n    Returns:\n        tensorflow.keras.Model: A Keras Model object that represents the discriminator network.\n\n    The discriminator architecture consists of:\n    1. A series of Conv2D layers that progressively reduce the spatial dimensions of the image while increasing the number of feature channels.\n    2. Each Conv2D layer is followed by a LeakyReLU activation function, which introduces non-linearity and prevents dead neurons.\n    3. An optional LayerNormalization step is commented out. Uncommenting it will normalize the activations across the features.\n    4. The final Conv2D layer reduces the output to a single channel.\n    5. A Flatten layer converts the 3D tensor into a 1D tensor.\n    6. A Dense layer produces a single score that represents the probability of the image being real or fake.\n    \"\"\"\n    # Define the input layer with the shape of the input image.\n    # This is the input to the discriminator network.\n    input_x_layer = Input(input_x_shape)\n    \n    # Apply a series of Conv2D layers to downsample the image.\n    # Each Conv2D layer reduces the spatial dimensions of the image.\n\n    # First convolutional layer\n    x = Conv2D(64, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(input_x_layer)\n    # x = LayerNormalization()(x)  # Uncomment this line to use LayerNormalization instead of BatchNormalization\n    x = LeakyReLU()(x)  # Apply Leaky ReLU activation function for non-linearity\n    \n    # Second convolutional layer\n    x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(x)\n    # x = LayerNormalization()(x)\n    x = LeakyReLU()(x)\n    \n    # Third convolutional layer\n    x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(x)\n    # x = LayerNormalization()(x)\n    x = LeakyReLU()(x)\n    \n    # Fourth convolutional layer\n    x = Conv2D(512, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(x)\n    # x = LayerNormalization()(x)\n    x = LeakyReLU()(x)\n    \n    # Final convolutional layer that reduces the output to a single channel\n    # This final layer does not have activation because we will apply a sigmoid function later to get the probability of the image being real or fake\n    x = Conv2D(1, (4, 4), strides=(1, 1), padding='same', use_bias=False, kernel_initializer=RandomNormal(mean=0.0, stddev=0.02))(x)\n    \n    # Flatten the 3D output into a 1D tensor\n    x = Flatten()(x)\n    \n    # Apply a Dense layer to output a single score indicating whether the image is real or fake\n    output = Dense(1)(x)\n    \n    # Create a model with the input and output layers defined above\n    model = Model(inputs=input_x_layer, outputs=output)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an instance of the CGAN generator model.\n# This model takes a noise vector as input and generates an image.\n# The noise vector has a dimensionality defined by NOISE_DIM (default is 100).\ngenerator = CGAN_generator()\ngenerator.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an instance of the CGAN discriminator model.\n# This model takes an image as input and outputs a score indicating whether the image is real or fake.\n# The input image shape is defined by the parameter (default is (TARGET_IMG_SIZE, TARGET_IMG_SIZE, 3)),\n# where TARGET_IMG_SIZE is the height and width of the image and 3 represents the RGB color channels.\ndiscriminator = CGAN_discriminator()\ndiscriminator.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Learning Rate Setup<section id='6'>","metadata":{}},{"cell_type":"code","source":"# Optimizers for the GAN models.\n# The Adam optimizer is used for both the discriminator (D_optimizer) and the generator (G_optimizer).\n# The learning rate for both optimizers is set to LR, which controls how much to adjust the weights in each iteration.\n# The beta_1 parameter is set to 0.5 to help stabilize the training process by controlling the exponential decay rate of the first moment estimates.\n\nD_optimizer = Adam(learning_rate=LR, beta_1=0.5)\nG_optimizer = Adam(learning_rate=LR, beta_1=0.5)\n\ndef learning_rate_decay(current_lr, decay_factor=DECAY_FACTOR):\n    '''\n    Calculate the new learning rate by applying a decay factor.\n    \n    The learning rate decay function reduces the learning rate gradually over time, which helps to stabilize training \n    as the model converges. The decay factor controls how quickly the learning rate decreases. This function ensures \n    that the learning rate does not fall below a minimum threshold defined by MIN_LR.\n    \n    Args:\n        current_lr (float): The current learning rate value.\n        decay_factor (float): The factor by which the learning rate is divided to get the new rate. Default is DECAY_FACTOR.\n        \n    Returns:\n        float: The updated learning rate, which is either the decayed rate or MIN_LR, whichever is higher.\n    '''\n    # Calculate the new learning rate by dividing the current learning rate by the decay factor.\n    new_lr = max(current_lr / decay_factor, MIN_LR)\n    return new_lr\n\ndef set_learning_rate(D_optimizer, G_optimizer, new_lr):\n    '''\n    Set a new learning rate for the optimizers.\n    \n    This function updates the learning rate for both the discriminator and generator optimizers. \n    It ensures that both optimizers use the same learning rate, which is important for balanced training.\n    \n    Args:\n        D_optimizer (tf.keras.optimizers.Optimizer): The optimizer used for training the discriminator.\n        G_optimizer (tf.keras.optimizers.Optimizer): The optimizer used for training the generator.\n        new_lr (float): The new learning rate to be set for both optimizers.\n        \n    Returns:\n        None: This function modifies the optimizers in-place and does not return a value.\n    '''\n    # Update the learning rate of the discriminator optimizer.\n    D_optimizer.learning_rate.assign(new_lr)\n    \n    # Update the learning rate of the generator optimizer.\n    G_optimizer.learning_rate.assign(new_lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Building Checkpoints<section id='7'>","metadata":{}},{"cell_type":"code","source":"# Define the path where the checkpoints (saved states of the model) will be stored.\n# This path includes a directory for TensorFlow checkpoints and a subdirectory named after the model.\ncheckpoint_path = os.path.join(\"checkpoints\", \"tensorflow\", MODEL_NAME)\n\n# Create a `tf.train.Checkpoint` object.\n# This object keeps track of the generator, discriminator, and both optimizers.\n# It allows us to save and restore their states during training.\nckpt = tf.train.Checkpoint(generator=generator,\n                           discriminator=discriminator,\n                           G_optimizer=G_optimizer,\n                           D_optimizer=D_optimizer)\n\n# Create a `tf.train.CheckpointManager` to manage the checkpoints.\n# This manager will save the checkpoints to the path defined earlier and keep up to 5 of the most recent checkpoints.\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# Check if there are any existing checkpoints in the checkpoint manager.\n# If there is at least one checkpoint, restore the latest one.\nif ckpt_manager.latest_checkpoint:\n    # Restore the state of the generator, discriminator, and optimizers from the latest checkpoint.\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    \n    # Extract the epoch number from the checkpoint file name.\n    # Checkpoints are named with the format 'ckpt-{epoch_number}', so split the name to get the epoch number.\n    latest_epoch = int(ckpt_manager.latest_checkpoint.split('-')[1])\n    \n    # Calculate the current epoch based on the latest checkpoint.\n    # MULTIPLY the epoch number from the checkpoint by the number of epochs between saves (SAVE_EVERY_N_EPOCH).\n    CURRENT_EPOCH = latest_epoch * SAVE_EVERY_N_EPOCH\n    \n    # Print a message indicating that the latest checkpoint has been restored and display the epoch number.\n    print (f'Latest checkpoint of epoch {CURRENT_EPOCH} restored!!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Function to Generate Images and Save it after each epoch<section id='8'>","metadata":{}},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input, figure_size=(12,6), subplot=(3,6), save=True, is_flatten=False):\n    '''\n    Generate images using a trained model and save or display them.\n\n    This function takes a trained model and a set of input data to generate images. It then plots these images\n    in a grid layout and optionally saves the generated images to a file.\n\n    Args:\n        model (tf.keras.Model): The trained model used to generate images. This model should be capable of taking\n                                the `test_input` and producing predictions (i.e., generated images).\n        epoch (int): The current epoch number of training. This is used to name the saved image file, indicating\n                     the state of the model at this epoch.\n        test_input (numpy.ndarray or tf.Tensor): The input data to feed into the model for generating images. \n                                                 This should be an array or tensor of noise vectors or other input \n                                                 that the model can process to produce images.\n        figure_size (tuple of int, optional): The size of the figure (in inches) on which the images will be plotted. \n                                              Default is (12, 6), which specifies a width of 12 inches and a height of 6 inches.\n        subplot (tuple of int, optional): The grid layout for displaying the images. It is specified as (rows, columns). \n                                          Default is (3, 6), which means the images will be arranged in 3 rows and 6 columns.\n        save (bool, optional): If True, the generated images will be saved to a file. Default is True.\n        is_flatten (bool, optional): If True, reshapes the predictions to a specific format before plotting. \n                                      Default is False. Set to True if the images are outputted in a flattened format.\n\n    Returns:\n        None: This function does not return any value but instead saves or displays the generated images.\n    '''\n    \n    # Generate images using the provided model and input data.\n    # The model's `predict` method takes the `test_input` and produces predictions (generated images).\n    predictions = model.predict(test_input)\n    \n    # If `is_flatten` is True, reshape the predictions array to the original image dimensions.\n    # This assumes that the images have been flattened (e.g., into a 1D array) and need to be reshaped.\n    if is_flatten:\n        predictions = predictions.reshape(-1, IMG_WIDTH, IMG_HEIGHT, 3).astype('float32')\n    \n    # Create a new figure with the specified size to plot the images.\n    fig = plt.figure(figsize=figure_size)\n    \n    # Loop through each generated image and plot it.\n    for i in range(predictions.shape[0]):\n        # Create a subplot for each image based on the specified grid layout.\n        axs = plt.subplot(subplot[0], subplot[1], i+1)\n        \n        # Display the image. The predictions are scaled from [-1, 1] to [0, 1] for proper visualization.\n        plt.imshow(predictions[i] * 0.5 + 0.5, cmap='gray')\n        \n        # Hide the axis for a cleaner display of images.\n        plt.axis('off')\n    \n    # If `save` is True, save the figure to a file. The filename includes the epoch number.\n    if save:\n        plt.savefig(os.path.join(OUTPUT_PATH, 'image_at_epoch_{:04d}.png'.format(epoch)))\n    \n    # Display the figure with the plotted images.\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the number of examples (images) to generate.\n# This sets how many images we want to produce in a single batch.\nnum_examples_to_generate = 18\n\n# Create a batch of random noise vectors to be used as input for the generator.\n# `tf.random.normal` generates a tensor of random values from a normal distribution.\n# The shape `[num_examples_to_generate, NOISE_DIM]` specifies that we want\n# `num_examples_to_generate` noise vectors, each with dimensionality `NOISE_DIM`.\nsample_noise = tf.random.normal([num_examples_to_generate, NOISE_DIM])\n\n# Generate images using the `generator` model with the `sample_noise` as input.\n# The function `generate_and_save_images` takes several arguments:\n# - `generator`: The trained generator model used to create images.\n# - `0`: The epoch number, used to label the image if saved (here it is 0 as we are not saving).\n# - `[sample_noise]`: The input data to feed into the generator. We wrap `sample_noise` in a list \n#   to match the expected input format for the function.\n# - `figure_size=(12,6)`: Specifies the size of the figure for plotting.\n# - `subplot=(3,6)`: Specifies the grid layout for the subplot (3 rows and 6 columns).\n# - `save=False`: Indicates that the generated images should not be saved to a file.\n# - `is_flatten=False`: Specifies that the generated images are not in a flattened format.\ngenerate_and_save_images(generator, 0, [sample_noise], figure_size=(12,6), subplot=(3,6), save=False, is_flatten=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Build the Training Step for Critic and Generator<section id='9'>","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef WGAN_GP_train_d_step(real_image, batch_size, step):\n    '''\n    Perform one training step for the discriminator in the Wasserstein GAN with Gradient Penalty (WGAN-GP) framework.\n\n    Args:\n        real_image (tf.Tensor): A batch of real images from the dataset, used to train the discriminator.\n        batch_size (int): The number of images in the batch.\n        step (int): The current training step, used for logging purposes.\n\n    Reference:\n        This implementation is inspired by the TensorFlow DCGAN tutorial:\n        https://www.tensorflow.org/tutorials/generative/dcgan\n    '''\n    # The @tf.function decorator converts this Python function into a TensorFlow graph function.\n    # This optimization allows TensorFlow to compile and optimize the function for performance.\n    # It improves execution speed by converting the function into a static computation graph,\n    # which TensorFlow can optimize and run more efficiently compared to the dynamic Python execution.\n    print(\"retrace\")\n\n    # Generate a batch of random noise vectors. These vectors will be used to create fake images.\n    noise = tf.random.normal([batch_size, NOISE_DIM])\n\n    # Generate random values for epsilon in the range [0, 1], with the same batch size as the real images.\n    # This is used to create interpolated images between real and fake images.\n    epsilon = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0, maxval=1)\n\n    ###################################\n    # Train the Discriminator (D)\n    ###################################\n\n    # Start recording gradients for the discriminator and gradient penalty computations.\n    # 'persistent=True' allows multiple gradient computations to be recorded, which is necessary for both the gradient penalty and discriminator loss calculations.\n    with tf.GradientTape(persistent=True) as d_tape:\n        # Start a nested gradient tape to compute the gradient penalty.\n        with tf.GradientTape() as gp_tape:\n            # Generate fake images using the generator model and the random noise.\n            fake_image = generator([noise], training=True)\n            \n            # Create mixed images by interpolating between real images and fake images using epsilon.\n            fake_image_mixed = epsilon * tf.dtypes.cast(real_image, tf.float32) + ((1 - epsilon) * fake_image)\n            \n            # Get the discriminator's predictions for the mixed images.\n            fake_mixed_pred = discriminator([fake_image_mixed], training=True)\n        \n        # Compute the gradient penalty to enforce the Lipschitz constraint.\n        # Calculate gradients of the discriminator's predictions with respect to the mixed images.\n        grads = gp_tape.gradient(fake_mixed_pred, fake_image_mixed)\n        \n        # Compute the norm (magnitude) of the gradients. (x1, x2, x3) ---> sqrt(x1^2 + x2^2 + x3^3)\n        grad_norms = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        \n        # Calculate the gradient penalty as the mean squared difference between the gradient norms and 1.\n        gradient_penalty = tf.reduce_mean(tf.square(grad_norms - 1))\n        \n        # Get the discriminator's predictions for the fake and real images.\n        fake_pred = discriminator([fake_image], training=True)\n        real_pred = discriminator([real_image], training=True)\n        \n        # Calculate the discriminator loss: the mean of fake predictions minus the mean of real predictions,\n        # plus the gradient penalty term scaled by LAMBDA.\n        D_loss = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred) + LAMBDA * gradient_penalty\n    \n    # Compute gradients of the discriminator loss with respect to the discriminator's trainable variables.\n    D_gradients = d_tape.gradient(D_loss, discriminator.trainable_variables)\n    # (0, 1, 2), (1, 2, 3) --> (0, 1), (1, 2), (2, 3)\n    # Apply the computed gradients to the discriminator's optimizer to update its weights.\n    D_optimizer.apply_gradients(zip(D_gradients, discriminator.trainable_variables))\n    \n    # Log the discriminator loss to TensorBoard every 10 steps for monitoring purposes.\n    if step % 10 == 0:\n        with file_writer.as_default():\n            tf.summary.scalar('D_loss', tf.reduce_mean(D_loss), step=step)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef WGAN_GP_train_g_step(real_image, batch_size, step):\n    '''\n    Perform one training step for the generator in the Wasserstein GAN with Gradient Penalty (WGAN-GP) framework.\n\n    Args:\n        real_image (tf.Tensor): A batch of real images from the dataset, used for the discriminator.\n        batch_size (int): The number of images in the batch.\n        step (int): The current training step, used for logging purposes.\n\n    Reference:\n        This implementation is inspired by the TensorFlow DCGAN tutorial:\n        https://www.tensorflow.org/tutorials/generative/dcgan\n    '''\n    # The @tf.function decorator converts this Python function into a TensorFlow graph function.\n    # This optimization allows TensorFlow to compile and optimize the function for performance.\n    # It improves execution speed by converting the function into a static computation graph,\n    # which TensorFlow can optimize and run more efficiently compared to the dynamic Python execution.\n    print(\"retrace\")\n\n    # Generate a batch of random noise vectors. These vectors will be used to create fake images.\n    noise = tf.random.normal([batch_size, NOISE_DIM])\n\n    ###################################\n    # Train the Generator (G)x\n    ###################################\n\n    # Start recording gradients for the generator's loss calculations.\n    with tf.GradientTape() as g_tape:\n        # Generate fake images using the generator model and the random noise.\n        fake_image = generator([noise], training=True)\n        \n        # Get the discriminator's predictions for the fake images.\n        fake_pred = discriminator([fake_image], training=True)\n        \n        # Calculate the generator loss as the negative mean of the discriminator's predictions for fake images.\n        # The generator's goal is to maximize this value, which is equivalent to minimizing its negative.\n        G_loss = -tf.reduce_mean(fake_pred)\n    \n    # Compute gradients of the generator loss with respect to the generator's trainable variables.\n    G_gradients = g_tape.gradient(G_loss, generator.trainable_variables)\n    \n    # Apply the computed gradients to the generator's optimizer to update its weights.\n    G_optimizer.apply_gradients(zip(G_gradients, generator.trainable_variables))\n    \n    # Log the generator loss to TensorBoard every 10 steps for monitoring purposes.\n    if step % 10 == 0:\n        with file_writer.as_default():\n            tf.summary.scalar('G_loss', G_loss, step=step)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. Training of WGAN-GP<section id='10'>","metadata":{}},{"cell_type":"code","source":"# Initialize the current learning rate to the initial learning rate specified by LR\ncurrent_learning_rate = LR\n\n# A flag to trace execution, typically used for debugging purposes\ntrace = True\n\n# Counter to track the number of discriminator (critic) updates\nn_critic_count = 0\n\n# Loop over each epoch in the training process\nfor epoch in range(CURRENT_EPOCH, EPOCHS + 1):\n    # Record the start time of the epoch for performance tracking\n    start = time.time()\n    \n    # Print the start of the epoch with the current epoch number\n    print('Start of epoch %d' % (epoch,))\n    \n    # Update the learning rate using the decay function\n    current_learning_rate = learning_rate_decay(current_learning_rate)\n    \n    # Print the updated learning rate for monitoring\n    print('current_learning_rate %f' % (current_learning_rate,))\n    \n    # Apply the updated learning rate to both the discriminator and generator optimizers\n    set_learning_rate(D_optimizer, G_optimizer, current_learning_rate)\n    \n    # Iterate over the training data in batches\n    for step, (image) in enumerate(train_data):\n        # Get the current batch size from the image tensor\n        current_batch_size = image.shape[0]\n        \n        # Train the discriminator (critic) on the current batch of real images\n        WGAN_GP_train_d_step(\n            image,\n            batch_size=tf.constant(current_batch_size, dtype=tf.int64),\n            step=tf.constant(step, dtype=tf.int64)\n        )\n        \n        # Increment the discriminator update counter\n        n_critic_count += 1\n        \n        # If the number of discriminator updates reaches the specified threshold (N_CRITIC)\n        if n_critic_count >= N_CRITIC:\n            # Train the generator on the current batch of images\n            WGAN_GP_train_g_step(\n                image,\n                batch_size=tf.constant(current_batch_size, dtype=tf.int64),\n                step=tf.constant(step, dtype=tf.int64)\n            )\n            # Reset the discriminator update counter\n            n_critic_count = 0\n        \n        # Print a dot every 10 steps to indicate progress in training\n        if step % 10 == 0:\n            print ('.', end='')\n    \n    # Clear the output of the Jupyter notebook cell to keep the output area tidy\n    clear_output(wait=True)\n    \n    # Generate and save images from the generator to visualize progress\n    # Use a consistent sample noise to compare progress over epochs\n    generate_and_save_images(\n        generator,\n        epoch,\n        [sample_noise],\n        figure_size=(12,6),\n        subplot=(3,6),\n        save=True,\n        is_flatten=False\n    )\n    \n    # If the current epoch is a multiple of SAVE_EVERY_N_EPOCH, save a checkpoint\n    if epoch % SAVE_EVERY_N_EPOCH == 0:\n        # Save the model's state to a checkpoint file\n        ckpt_save_path = ckpt_manager.save()\n        # Print the path where the checkpoint was saved\n        print('Saving checkpoint for epoch {} at {}'.format(epoch, ckpt_save_path))\n    \n    # Print the time taken for the current epoch\n    print('Time taken for epoch {} is {} sec\\n'.format(epoch, time.time() - start))\n\n# Save a final checkpoint at the end of training\nckpt_save_path = ckpt_manager.save()\n# Print the path where the final checkpoint was saved\nprint('Saving checkpoint for epoch {} at {}'.format(EPOCHS, ckpt_save_path))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11. Compare Generated Data with Real Data<section id='11'>","metadata":{}},{"cell_type":"code","source":"# Check for perfect square\nif NUM_IMAGES < 0:\n    raise ValueError(\"Number must be non-negative\")\nif not np.sqrt(NUM_IMAGES).is_integer():\n    raise ValueError(f\"{NUM_IMAGES} is not a perfect square\")\n\n# Create a new batch of random noise vectors as input for the generator.\n# This noise will be used to generate images and assess the model's performance.\n# The batch size is set to 64, and each noise vector has the dimension NOISE_DIM.\ntest_noise = tf.random.normal([NUM_IMAGES, NOISE_DIM])\n\n# Generate images using the generator model with the new batch of random noise vectors.\n# The generator takes the noise as input and produces corresponding fake images.\nprediction = generator.predict(test_noise)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_grid(images, fig, images_to_generate):\n    '''\n    Display a grid of images in a specified matplotlib figure.\n\n    Args:\n        images (numpy.ndarray): A numpy array containing images to be displayed.\n            The array should have shape (N, H, W, C), where N is the number of images,\n            H is the height of each image, W is the width of each image, and C is the number of channels (3 for RGB).\n        fig (matplotlib.figure.Figure): The matplotlib figure object where the image grid will be plotted.\n\n    Description:\n        This function takes a batch of images and a matplotlib figure object, and displays the images in an 8x8 grid layout.\n        Each subplot in the grid is created using `fig.add_subplot`, and the images are shown without axis ticks.\n        The images are clipped to the range [0, 1] and scaled from the range [-1, 1] if necessary.\n    '''\n    # Iterate over the first 64 images in the batch.\n    for i in range(images_to_generate):\n        # Add a subplot to the figure at the (i + 1)th position in an 8x8 grid.\n        axs = fig.add_subplot(int(np.sqrt(images_to_generate)), int(np.sqrt(images_to_generate)), i + 1)\n        # Remove x and y axis ticks for the subplot.\n        axs.set_xticks([])\n        axs.set_yticks([])\n        # Display the image in the current subplot.\n        # Clip the image values to be between 0 and 1 for correct visualization.\n        axs.imshow(images[i] * 0.5 + 0.5, cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the real images from the dataset\n\n# Create a new matplotlib figure with a specified size of 12x12 inches.\n# This will be used to display the grid of images.\nfig1 = plt.figure(figsize=(12,12))\n\n# Call the image_grid function to add the images to the figure.\n# `sample_img.numpy()[:NUM_IMAGES]` retrieves the first NUM_IMAGES images from the sample image batch,\n# which are then passed to the image_grid function to be arranged in an 8x8 grid.\nn = 1\nimage_grid(sample_img[:NUM_IMAGES], fig1, NUM_IMAGES)\n\n# Display the figure with the plotted images.\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the fake images generated by the model from the last epoch\n\n# Create a new matplotlib figure with a specified size of 12x12 inches.\n# This will be used to display the grid of generated images.\nfig2 = plt.figure(figsize=(12,12))\n\n# Call the image_grid function to add the generated images to the figure.\n# `prediction` contains the fake images generated by the model.\n# These images are passed to the image_grid function to be arranged in an 8x8 grid.\nimage_grid(prediction, fig2, NUM_IMAGES)\n\n# Display the figure with the plotted generated images.\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the generator model\ngenerator.save('generator_wgan_gp.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Path to your data and the zip file\ndata_folder = '/kaggle/working/'\nzip_file = '/kaggle/working/saved_data.zip'\n\n# Create a zip file\nwith zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:\n    for root, dirs, files in os.walk(data_folder):\n        for file in files:\n            file_path = os.path.join(root, file)\n            zf.write(file_path, os.path.relpath(file_path, data_folder))\n\nprint(f\"Data zipped and saved to {zip_file}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}